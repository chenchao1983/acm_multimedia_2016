=======================
Feedback to reviewer #1
=======================

1. Do you observe any content dependency in Fig 4? Also, there are different
numbers of points for the different noise power values. Is that due to the
random selection of distorted versions (9 out of 12) for each clip? Please
clarify. Clusters of points can be observed in Fig. 8 for the VQM and STMAD
metrics.  Is that due to content dependency? Please analyze. Could you also
provide different color and pattern codes for the different clips?

Thanks for the suggestions. In the revised version of the paper, we use
different types of markers in Figure 4 and figure 8 to show the data points
corresponding to different video contents. We can observe clear content
dependency in the mean opinion scores. Noises are more visible for videos with
low brightness (e.g. talkingman) than videos with bright scenes (e.g. snow). We
also note that complex textures (e.g. beach) and dynamic scenes (e.g. goose)
tend to mask noises.

When we create the clips for each video, we did not try to have
a uniform distribution over the spectrum of noise power. Instead, for each
video, we manually select the now power such that the clips created for
each video content covers the full range of perceptual noise level. We clarified
this in the revised paper.

2. You should also clarify how you obtained values for the metric parameters.
You mention in Section 4 that you tuned the stabilizing constant alpha and the
parameter p using your video database. But how did you obtain the parameter
values psi and phi in Section 3.4? Also using the video database? Please
clarify.

Thanks for the question. The parameter psi and phi are also tuned using our
database. We have added clarification in the revised paper as well.

3. You should justify why you apply a double mapping between distortion D and
MOS(D) and MOS(D) and MOSpred. I suppose that you obtained the mapping
coefficients between D and MOS(D) using the subjective test results as target
values. Then, you map MOS(D) to MOSpred again using the subjective test results.
Why are you not directly mapping D to MOSpred? Also, with this double mapping
using twice the subjective test results as target values you advantage your
metric in comparison to the other metrics (PSNR, VQM, etc.). Please discuss this
statement.

Sorry for the confusion. We did not apply a double mapping as D -> MOS(D) ->
MOS_pred. The polylog mapping from D -> MOS(D) is to give readers a simple
non-linear mapping to predict MOS using our metric. The MOS_pred is directly
derived from D using the nonlinear model given in equation (13).  It is not
derived from MOS(D). MOS_pred is only used for the fair comparison with other
metrics such as SSIM and STMAD because equation (13) is the standard model for
metrics comparison. In the revised version, we clarified this point.

Also, in the revised paper. We replaced our polylog nonlinear mapping with a
linear mapping, which gives us much simpler mapping without sacrificing
accuracy. The linear correlation achieved by the linear mapping is 0.9490.

4. Generally speaking, you should mention that your metric is advantaged over
the other metrics by being trained and validated on the same dataset. Of course,
you provide cross-validation results as well, which makes the comparison between
metrics a bit fairer. However, your database is homogeneous in terms of
degradation (it contains spatially correlated noise degradation only). This
prevents the metric to generate outliers during the cross-validation procedure.
Also, for a fairer comparison with the other metrics, you should provide the
same table as Table 1 when the metric is cross-validated using 60% of the
database. In that case and since you are using 1000 random training/test
partitions, you would provide the average SROCC, LCC and VoR values. Finally, I
would be interested clusters of points in seeing performance results when
conducting the cross-validation at content level, that is, when training on N-k
clips and validating on k clips, where N is the number of clips (6) and with
different values for k.

Thanks for the comments. In the revised paper, we included a new table compares
the performance of our metric with PSNR, SSIM, VQM and STMAD on test sets when
60% of videos are used for training.

We expect the algorithm to show worse performance in content-wise
cross-validation because we designed our training set such that each video
content presents different spatial-temporal characteristics from the others.
Removing any content from training would make the training set much less
representative and make the output of our metric biased. In this sense, our
training set does not have redundant contents of similar spatial-temporal
characteristics. Given the limited number of videos can be involved in a test,
we must make our training set representative and compact.

5. Finally, YouTube videos do not contain spatially correlated noise only but
degradation such as blur or blockiness as well, even before video transmission.
Do you plan to develop a metric which covers several types of degradation? If
so, what are the next steps you plan to follow? Note that having such a metric
covering various distortions would make the comparison with other metrics such
as VQM even fairer since these metrics have been designed for covering various
types of degradation and not a single one such as spatially correlated noise.
Please comment.

Thanks for asking. Aside from the noise metric, we have developed metrics for
banding, blocking and blur artifacts. Our long term plan is to study how to
combine those metrics for videos that present all these artifacts in the same
time. In the revised paper, we added comments in the conclusion section.

6. (Questions requires feedback) You are using ratio between P and I frame sizes
and I frame sizes and maximum I frame size to characterize the spatial-temporal
complexity of the contents. I would be curious to know a) why you are not using
the commonly used Spatial and Temporal Information (SI and TI) values from ITU-T
Rec. P.910 b) how you metrics correlate the SI and TI indicators.  Note that the
reader may also be interested in such analysis, so please consider including it
in the paper.

An important motivation for us to use I/P frame size is that we have access to
the encoding logs of YouTube transcoding service, which readily recorded the I
frame size and P frame size for all the videos uploaded to YouTube. We agree
that SI and TI would be a good choice, but I frame and P frame size also suffice
for our purpose.

7. Typos
Thanks for pointing out the errors. We have fixed all of them and make another
round of proofreading to make sure the paper is grammatical.

=======================
Feedback to Reviewer #2
=======================
1. My main critique is that the evaluated dataset has only applied noise as
distortion, however other kinds of distortion, e.g. quantization and packet-loss
are common. The authors show that their metric can predict user ratings based
from noise quite well. However it is unknown how their algorithm performs in the
presence of other factors. The compared algorithm are often tuned towards these
parameters, thus the comparison seems a little bit unfair. These aspects should
be better qualified, especially in the conclusion of the paper.

Thank you for the comments. Our training set only involves noise. While we were
developing this algorithm, we also created several metrics for blockiness,
banding and blur. In the future, we will combine all these metrics and create a
metric for the overall quality of videos suffering multiple types of
distortions. In the revised paper, we clarified this point as our future work.

2. In a minor note, in section 3.2, "On the other hand, for regions of higher
than mid-grey luminance, the noise visibility decreases with luminance, which is
also known as Weberâ€™s law" This could be interpreted in a way that Weber's law
is about luminance-noise relation specifically, while it is about relative
stimulus intensity in general.  A small reformulation would clarify this.

Thanks for pointing out this issue. We agree with your comments. In the revised
paper, we replace "also known as Weber's law" with 'is due to Weber's law'.

3. Section 2.2, end of second paragraph, "The sample frames of the six clips are
shown in Fig. 3." Should probably read something like "Sample frames from six of
the clips..."

Thanks for the comment. We have fixed this in the revised paper.


=======================
Feedback to Reviewer #3
=======================

1. Only a deeper discussion of Figure 5 and a description of further necessary
activities is missing in my opinion.  How did you define the architecture of
proposed video metric (Figure 5)?

Thanks for the suggestions. We added a detailed descriptions of Figure 5 and
some discussion of our future work in the revised paper.

2. Do you know from other trials (literature describing them) to build or define
a no-reference model or metric?  Are there any plans for further proof of
concept?

Thanks for asking.  We note that VQEG is actively investigating a non-reference
metric for IMAGE quality. They developed a new metric called VQEG Image Quality
Evaluation Tool (VIQET). The metric is based on crawd sourcing image data. For
no-reference VIDEO quality assessment, we don't know much about the recent
advances except the references we cited in the paper.

For further concept proof, we started to run our noise metric on 1% of YouTube
videos to track the noise level. From these videos, we randomly selected a set
of clips such that their MOS predicted by our metric is 30, 40, ..., 90.  We
found that Their perceptual qualities agree well with the output of the proposed
metric. We believe our metric is reliable and it will be very helpful for
YouTube and other video service providers to optimize its denoising pipeline.
Due to the limititation on the length of paper, we were not able to include
these resutls. But we added some discussion in the conclusion of the paper.


3. Section 1.: 2nd paragraph: Abbreviations SSIM, MOVIE, STMAD are not
introduced or explained.

In the revised version, we added the full names for these metrics to make it
more readable.

4. 3rd paragraph: Pristine... Do you mean reference or original material?

Thanks for the question. In the revised version, we replace pristine with
'reference' to make it clear.

5. Section 2.1: No introduction of PSD although later used as abbreviation

In the revised paper, we added the following reference to explain the concept of
PSD.

Alan V. Oppenheim, Ronald W. Schafer, and John R. Buck. 1999. Discrete-Time
Signal Processing (2nd Ed.). Prentice-Hall, Inc., Upper Saddle River, NJ, USA.


6. Section 2.2: 2nd paragraph: Why resize from 4K to 1080p? Earlier it is
written that different resolutions are investigated.

Thanks for the comment and sorry for the confusion. In our analysis of realistic
noise, we investigated a comprehensive video set of 46000 videos, which involves
all types of video resolutions.  In our subjective tests, we select 6 video
contents from a set of 3226 high quality 4K videos uploaded to YouTube. Then we
downsize the selected 4K video into 1080p using ffmpeg so that we can display
the two 1080p videos side by side on our 4K screen. In the revised paper, we
clarified this point for readers.

7. How/Why did you define the values of block size and temporal lenght? Why T=3?

The block size and temporal length is inherited from the PSD estimation
algorithm in

A. Kokaram, D. Kelly, H. Denman and A. Crawford, "Measuring noise correlation
for improved video denoising," 2012 19th IEEE International Conference on Image
Processing, Orlando, FL, 2012, pp. 1201-1204.

We clarified this point in the revised paper.

8. Other Typos
Thanks for pointing out the typos. We have fixed them in the revised version of
the paper.

=======================
Feedback to Reviewer #4
=======================
1. Why is video resized? How is it resized?

Thanks for the comments. We selected our test clips from thousands of 4K videos
with more than 100Mbps encoding bitrates. These videos are captured by high-end
devices and have good quality. Then, we down-size the selected 4K video contents
to 1080p format using ffmpeg scaling filter with the following command. The
output is saved as yuv raw videos without re-encoding thus there is not quality
degradation introduced in this process.

We choose to use 1080p videos in our experiment for two reasons:
1) Because we did a double-stimulus test, by resizing the video to 1080p, we can
use one 4K screen to show two 1080p videos side by side to our subjects.
2) 1080p is one of the most popular formats uploaded to YouTube.

In the revised paper, we have made clarification on why/how we resized the
videos.

2.I would not refer to SSIM, MOVIE and STMAD as metrics for video quality
prediction, but video quality assessment or video quality evaluation.  Page 2:
"A subjective test is performed". Also in the following I would refer to "Test
session" or "series of subjective tests" rather than just test.

Thanks for the suggestions. We have revised the paper accordingly.


3.Equation (1) presents an odd notation / definition: rather than to a Gaussian
random field, it should refer to the realization of a random variable with
Gaussian distribution. The notation in the second column is not correct (the
symbol used means "for each" not "each").  More correct to say that \sigma_2
characterizes the power of the noise (rather than \sigma).  It -> the
method/algorithm

Thanks for pointing out these errors. We have corrected them issues in our paper.


4.The section could have reported an example with figures to illustrate the
procedure.

We are guessing that you are suggesting that we add some examples for the noise
power spectrum density estimation procedure. We agree that it would greately
improve the quality of the paper. However, due to the limitation on the length
of the paper, we were not able to include more figures. Instead, we asked
readers to refer to the following paper where several examplary figures are
included there.

A. Kokaram, D. Kelly, H. Denman and A. Crawford, "Measuring noise correlation
for improved video denoising," 2012 19th IEEE International Conference on Image
Processing, Orlando, FL, 2012, pp. 1201-1204.

5. Pframe size / Iframe size as indicator of temporal complexity. Any reference /
validation for this?

Thanks for the comments. An important motivation for us to use I/P frame size is
that we have access to the encoding logs of YouTube transcoding service, which
readily recorded the I frame size and P frame size for all the videos uploaded
to YouTube. In the revised paper, we added the following reference which
justified the use of P/I frame size as spatial/temporal complexity.

Chao Chen, Sasi Inguva, Andrew Rankin and Anil Kokaram, "A Subjective Study for
the Design of Multi-resolution ABR Video Streams with the VP9 Codec", SPIE
Electronic Imaging, Human Visual Perception, 2016

6. Same QP to make frame size comparable - not true that same QP makes frame size
comparable, but perhaps what the authors mean is that, if the same QP is used,
the size of frames could be an indication of the complexity of the sequence
(motion complexity / texture complexity).

Thanks for the suggestions. We agree that same QP does not necessarily mean that the
frame size is comparable. In the revised paper, we have removed this claim. The
constant QP only makes frame size a better indicator of spatial-temporal
complexity.

7.Fitting is performed, but the metric is not tested on "unseen data" The
"cross-validation" tests mentioned do not replace that.

To further justify the performance of the proposed metric on "unseen data", we
ran our noise metric on 1% of YouTube videos. From these videos, we randomly
selected a set of clips such that their MOS predicted by our metric is 30, 40,
..., 90.  We found that their perceptual qualities agree well with the output of
the proposed metric. We believe our metric is reliable and it will be very
helpful for video service providers such as YouTube to optimize their video
processing pipelines.

The results above is included in the revised paper.


8.What is the difference between fig 8 and 9 apart from scale? I would omit either
of them.

Yes, we agree. We have removed figure 9.

9.What described in the text as figure 4 is actually figure 10 (without caption).

Thanks for pointing out this problem. We have addeed captions for figure 10 and
corrected the reference to the figure.

